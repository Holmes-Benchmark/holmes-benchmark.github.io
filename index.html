---
layout: home
title: Holmes
subtitle: Benchmark Linguistic Knowledge in Language Models
---

<div style="text-align:center">
When explicitly assessing language models (LMs) for abilities like language understanding or factual knowledge, we credit them linguistic proficiency as they provide linguistically acceptable and coherent responses.
However, such implicit assessments of linguistic knowledge are entangled with evaluation protocols, like specific instructions.
With the Holmes ðŸ”Ž benchmark, we address recently raised calls to disentangle linguistic knowledge from other evaluations and verify it comprehensively and explicitly within the representations of LMs.
Specifically, Holmes ðŸ”Ž extensively consolidates existing resources to form over 200 probing datasets covering 74 English linguistic phenomena and rigorously evaluate 50 LMs.
Generally, LMs tend to strongly encode formal knowledge about morphology and syntax and weaker functional information about semantics, reasoning, or discourse.
Further analysis reveals that model size, architecture, and instruction fine-tuning (IT) influence linguistic knowledge.
Notably, we show for the first time how IT affects the emergence of linguistic knowledge, particularly positively for large LMs.
Summing up, with these empirical insights and by releasing evaluation code, detailed results on a wide range of up-to-date LMs, and a lightweight version called FlashHolmes âš¡, our work significantly contributes to the field of LM evaluation research.
</div>