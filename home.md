---
layout: home
title: Holmes
subtitle: Benchmark the Linguistic Competence of Language Models
---

![Drag Racing](assets/img/benchmark.jpg)

Holmes ðŸ”Ž is a research project focusing on assessing the linguistic competence of language models.
Along with the benchmark itself, we provide different resources.


## ðŸ“š Benchmark
The Holmes ðŸ”Ž benchmark features over 200 dataset covering 66 phenomena for *morphology*, *syntax*, *semantics*, *reasoning*, and *discourse*.
Using classifier-based probing, Holmes ðŸ”Ž directly assesses the linguistic competence of language models without tangling them with other abilities, like following provided instructions in prompting-based evaluations.
Find an overview of the insights [here](https://holmes-benchmark.github.io/insights/) and more details in our paper.

## ðŸ”¥ Evaluation Code
Run the evaluation of your favorite language model quickly and easily. We provide code to run Holmes ðŸ”Ž or FlashHolmes âš¡ with no more than one command.
Find it [here](https://github.com/Holmes-Benchmark/holmes-evaluation)

## ðŸš€ Leaderboard
The **Holmes Leaderboard** provides an interactive overview of evaluating over 50 different language models for Holmes ðŸ”Ž and its counterpart FlashHolmes âš¡ - optimized for efficiency.
Find it [here](https://holmes-leaderboard.streamlit.app)

## ðŸ”Ž Interactive Exploration
Using the **Holmes Explorer**, one can delve into more detailed results by comparing single datasets, phenomena, or phenomena types.
Find it [here](https://holmes-explorer.streamlit.app)