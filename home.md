---
layout: home
title: Holmes
subtitle: Benchmark the Linguistic Competence of Language Models
---

Holmes ðŸ”Ž is a research project focusing on the assessment of the linguistic competence of language models.
Along with the benchmark itself, we provide different resources.

![Drag Racing](assets/img/benchmark.jpg)

# ðŸ“š Benchmark
The Holmes ðŸ”Ž benchmark features over 200 dataset covering 66 phenomena for *morphology*, *syntax*, *semantics*, *reasoning*, and *discourse*.
Using classifier-based probing, Holmes ðŸ”Ž directly assess the linguistic competence of language models without tangling them with other abilities, like following provided instructions in prompting-based evaluations.
Find an overview of the insights [here](https://holmes-benchmark.github.io/insights/) and more details in our [paper](todo).


# ðŸš€ Leaderboard
The **Holmes Leaderboard** provides an interactive overview of evaluating over 50 different language models for Holmes ðŸ”Ž and its counterpart FlashHolmes âš¡ - optimized for efficiency.

Find it [here](https://holmes-leaderboard.streamlit.app)

# ðŸ”Ž Interactive Exploration
Using the **Holmes Explorer**, one can delve into more detailed results by comparing single datasets, phenomena or phenomena types.

Find it [here](https://holmes-explorer.streamlit.app)

# ðŸ”¥ Evaluation Code
Enable the fast evaluation of your favorite language model, we provide code to run Holmes ðŸ”Ž or FlashHolmes âš¡ with no more than one command.

Find it  [here](https://github.com/Holmes-Benchmark/holmes-evaluation)